---
title: 'Homework 2: Body Fat Modeling'
output:
  pdf_document:
    fig_height: 4
    fig_width: 6
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA, warning=FALSE, message=FALSE, fig.width =6, fig.height = 4, fig.align = 'center')
```

```{r libraries, include=FALSE}
library(tidyverse)
library(corrplot)
library(leaps)
library(MASS)
library(lars)
library(pls)
library(kableExtra)
```

## Introduction

The purpose of this report is to explore the body fat dataset in the "faraway" library of R, and also try and predict the Brozek's equation for percentage of body fat. We will explore different linear regression models using different variable selection, regularization, and dimension reduction methods.

## Exploratory Data Analysis

The dataset $fat$ has 252 observations and 18 variables, 1 variable being the response variable ($brozek$) and the other 17 will be used as potential predictors. Before we analyze the dataset we will split it into training and testing sets. We'll randomly select 10% of the full dataset and set it aside to be used for testing our models. The other 90% will be used for exploratory data analysis and training the various prediction methods. 

Let's look at a correlation plot of the training set to see correlations between the response variable and the predictors as well as the predictors with each other.

```{r load_data}

fat <- read.table(file = "fat.csv", sep = ",", header = TRUE)

n = dim(fat)[1]
n1 = round(n/10)
set.seed(12)
flag = sort(sample(1:n, n1))

fat1train = fat[-flag,]
fat1test = fat[flag,]

```

##### Correlation Matrix Plot

```{r corrplot}
fat.mtx <- cor(fat1train)

corrplot::corrplot(fat.mtx, type = "upper", diag = FALSE)
```

Looking across the top row we can see how the predictors are correlated with $brozek$. Two predictors, $siri$ and $density$, look to be highly correlated with $brozek$. $Siri$ has a strong positive correlation while $density$ has a strong negative correlation. We can see a couple others predictors with high correlations with $brozek$, such as $weight$, $adipos$, $chest$, $abdomen$, and $hip$. We can also see that $weight$ and $adipos$ have strong correlations with many of the other predictors. Let's isolate these predictors and look at some scatter plots between them.

##### Variable Scatter Plots

```{r scatplot}
pairs(fat1train[, c(1:3, 5, 7, 10, 11, 12)], lower.panel = NULL)
```

Looking at $siri$ and $density$ we see scatter plots that are very close to being straight lines, again implying very strong correlation with $brozek$. With the other predictors we can also see good relationships with $brozek$. There seem to be a lot of relationships between the variables in this dataset, so this dataset seems to be highly correlated.

## Methods

This section will detail each method used. The following methods will be used (and shorthand used throughout report):

* Linear regression using all predictors (LRALL)
* Linear regression using the best subset of 5 predictors (LR5)
* Linear regression with stepwise variable selection using AIC (STEP)
* Ridge regression (RIDGE)
* LASSO regession (LASSO)
* Principal component regression (PCR)
* Partial least squares (PLS)

For each method we will fit it using the training set and then make predictions on the test set. We will then initially evaluate each method by looking at the mean squared error on the test set.

### Linear regression using all predictors (LRALL)

The first method is a linear regression model with no variable selection, so all predictors will be considered in the model. 

```{r LR_all}
model1 <- lm(brozek ~ ., fat1train)


model1.predict <- predict(model1, fat1test[,2:18])
model1.te <- mean((fat1test[,1] - model1.predict)^2)

```

### Linear regression using the best subset of 5 predictors (LR5)

This method we look to find the best subset of 5 predictors, then we will create a linear regression model using the selected predictors. To determine the best subset of 5 predictors, we create multiple models with different combinations of 5 predictors. We then select the model that has the lowest RSS (residual sum of squares) on the training set. The best subset of 5 predictors is $siri$, $density$, $thigh$, $knee$, and $forearm$.

```{r LR_5}
lm.subset <- regsubsets(brozek ~ ., fat1train, nbest= 10, really.big= TRUE)
model2.models <- summary(lm.subset)$which
model2.models.size <- as.numeric(attr(model2.models, "dimnames")[[1]])
model2.models.rss <- summary(lm.subset)$rss

op5 <- which(model2.models.size == 5)
flag2 <- op5[which.min(model2.models.rss[op5])]

mod2selectedmodel <- model2.models[flag2,]
mod2Xname <- paste(names(mod2selectedmodel)[mod2selectedmodel][-1], collapse="+")
mod2form <- paste ("brozek ~", mod2Xname)
model2 <- lm(as.formula(mod2form), data = fat1train)

model2.predict <- predict(model2, fat1test[,2:18])
model2.te <- mean((fat1test[,1] - model2.predict)^2)
```

### Linear regression with stepwise variable selection using AIC (STEP)

Next we will use stepwise variable selection to find a model using AIC. Instead of limiting the model to only 5 predictors like the previous method, stepwise will add and subract variables until it finds the best model based on AIC. Using stepwise we came up with a model using the following predictors: $siri$, $density$, $weight$, $adipo$s, $free$, $thigh$, $knee$, $biceps$, $forearm$, and $wrist$. So the stepwise model uses the same 5 predictors we found in our best subset of 5, but also includes 5 other predictors that could help explain the variation in $brozek$.

```{r LR_stepwise}
model3  <- step(model1, trace = FALSE)

model3.predict <- predict(model3, fat1test[,2:18])
model3.te <- mean((fat1test[,1] - model3.predict)^2)
```

### Ridge regression (RIDGE)

Next we will use ridge regression to come up with the model coefficients. Ridge uses L2 regularization to, in effect, shrink the coefficients to the "true" parameter values. Ridge regression is not used for variable selection, as coefficient values will never go to zero. Instead all predictors are used, and this method can help predictions when multicollinearity is present like it is in our dataset. 

```{r Ridge}

model4 <- lm.ridge(brozek ~ ., data = fat1train, lambda= seq(0,100,0.001))


indexopt <-  which.min(model4$GCV)

ridge.coeffs = model4$coef[,indexopt]/ model4$scales
intercept = -sum(ridge.coeffs  * colMeans(fat1train[,2:18] )  ) + mean(fat1train[,1])

model4.predict <- as.matrix(fat1test[,2:18]) %*% as.vector(ridge.coeffs) + intercept
model4.te <- mean((fat1test[,1] - model4.predict)^2)
```

### LASSO regession (LASSO)

LASSO is the next method we will use. Like ridge LASSO is a type of regularization, but uses L1 regularization. LASSO can sometimes eliminate coefficients all together and work as a pseudo variable selection tool, usually yielding sparse models. With our training set many predictors are eliminated, leading to a model with just the following predictors left: $siri$, $density$, $age$, $thigh$, $knee$, $biceps$, $forearm$, and $wrist$.

```{r LASSO}
model5 <- lars(as.matrix(fat1train[,2:18]), fat1train[,1], type= "lasso", trace= FALSE)

Cp1  <- summary(model5)$Cp
index1 <- which.min(Cp1)

LASSO.coeffs <- coef(model5)[index1,]

LASSOintercept = mean(fat1train[,1]) -sum(LASSO.coeffs  * colMeans(fat1train[,2:18]))

model5.predict <- as.matrix(fat1test[,2:18]) %*% as.vector(LASSO.coeffs) + LASSOintercept
model5.te <- mean((fat1test[,1] - model5.predict)^2)
```

### Principal component regression (PCR)

This method is a dimensionality reduction tool. It transforms the predictors into components, which are linear combinations of the predictors. You can then use a subset of components (if desired) to reduce the dimensions in your model while still trying to keep as much variance in the response variable as possible. When using PCR on our training dataset, the optimal number of components was 17 which amounts to using the full dataset.  

```{r PCR}
set.seed(33)
model6 <- pcr(brozek~., data=fat1train, validation="CV")

ncompopt <- which.min(model6$validation$adj)

model6.predict <- predict(model6, ncomp = ncompopt, newdata = fat1test[,2:18])
model6.te <- mean((fat1test[,1] - model6.predict)^2)
```

### Partial least squares (PLS)

Partial least squares is similar to PCR, but instead of maximizing the variance of the predictors it maximizes the correlation between the predictors and the response. It can be used for dimensionality reduction as well, but like PCR on our training set the optimized number of components is 17, so once again the full dataset is used.

```{r PLS}
set.seed(33)
model7 <- plsr(brozek~., data=fat1train, validation="CV")

mod7ncompopt <- which.min(model7$validation$adj)

model7.predict <- predict(model7, ncomp = mod7ncompopt, newdata = fat1test[,2:18])
model7.te <- mean((fat1test[,1] - model7.predict)^2)
```

## Results

Below is a table of each method and their subsequent mean squared error on the test set. So on our test set the LASSO and LR5 method performed the best. It's worth noting that these models are the two that had the lowest number of predictors (8 and 5 respectively) compared to the others. So if seems that adding more predictors only ended up adding noise to the model that did not do as well in predicting the response for our test set.

```{r results1}
methods <- c("LRALL", "LR5", "STEP", "RIDGE", "LASSO", "PCR", "PLS")
errors <- c(model1.te, model2.te, model3.te, model4.te, model5.te, model6.te, model7.te)

table <- data.frame(methods, errors)

kable(table, col.names = c("Methods", "Mean Squared Error"), booktabs = T) %>%
  kable_styling(latex_options = c("striped", "HOLD_position"), full_width = F)
```

### Cross-Validation

Our datasets are relatively small, with our test set only containing 25 records. To better evaluate our methods outside of our 25 record test set we will use Monte Carlo Cross-Validation. This algorithm randomly selects new training and test sets (90/10 split as before), create, train, and test each model in the same manner we did above with these new datesets, and then repeat the process with a new randomly selected training and test set. So in the end we will have trained our models on 100 different datasets, and then tested them on 100 different test sets, leaving us with 100 different testing errors. We will then compute and compare the average performance of each method.

```{r MCCV}
B = 100
TEALL = NULL

set.seed(33)
for (b in 1:B){
  partition <- sort(sample(1:n, n1))
  fattrain <- fat[-partition,]
  fattest <- fat[partition,]
  
  ##LM ALL
  lm <- lm(brozek ~ ., fattrain)

  lm.predict <- predict(lm, fattest[,2:18])
  te1 <- mean((fattest[,1] - lm.predict)^2)
  
  ##LM 5
  subset <- regsubsets(brozek ~ ., fattrain, nbest= 10, really.big= TRUE)
  lm5.models <- summary(subset)$which
  lm5.models.size <- as.numeric(attr(lm5.models, "dimnames")[[1]])
  lm5.models.rss <- summary(subset)$rss
  
  op <- which(lm5.models.size == 5)
  flag2 <- op5[which.min(lm5.models.rss[op])]
  
  modselectedmodel <- lm5.models[flag2,]
  modXname <- paste(names(modselectedmodel)[modselectedmodel][-1], collapse="+")
  modform <- paste ("brozek ~", modXname)
  lm5 <- lm(as.formula(modform), data = fattrain)
  
  lm5.predict <- predict(lm5, fattest[,2:18])
  te2 <- mean((fattest[,1] - lm5.predict)^2)
  
  ##LM Stepwise
  step.model <- step(lm(brozek ~ ., fattrain), trace = FALSE)

  step.predict <- predict(step.model, fattest[,2:18])
  te3 <- mean((fattest[,1] - step.predict)^2)
  
  ##Ridge
  ridge <- lm.ridge(brozek ~ ., data = fattrain, lambda= seq(0,100,0.001))
  indexopt <-  which.min(ridge$GCV)
  
  r.coeffs = ridge$coef[,indexopt]/ ridge$scales
  r.intercept = -sum(r.coeffs  * colMeans(fattrain[,2:18] )  ) + mean(fattrain[,1])

  
  ridge.predict <- as.matrix(fattest[,2:18]) %*% as.vector(r.coeffs) + r.intercept
  te4 <- mean((fattest[,1] - ridge.predict)^2)
  
  ##LASSO
  lasso <- lars(as.matrix(fattrain[,2:18]), fattrain[,1], type= "lasso", trace= FALSE)
  Cp1  <- summary(lasso)$Cp
  index1 <- which.min(Cp1)
  
  L.coeffs <- coef(lasso)[index1,]
  L.intercept = mean(fattrain[,1]) -sum(L.coeffs  * colMeans(fattrain[,2:18]))
  
  lasso.predict <- as.matrix(fattest[,2:18]) %*% as.vector(L.coeffs) + L.intercept
  te5 <- mean((fattest[,1] - lasso.predict)^2)
  
  ##PCR
  pcr <- pcr(brozek~., data=fattrain, validation="CV")
  
  ncompopt <- which.min(pcr$validation$adj)
  
  pcr.predict <- predict(pcr, ncomp = ncompopt, newdata = fattest[,2:18])
  te6 <- mean((fattest[,1] - pcr.predict)^2)
  
  ##PLS
  pls <- plsr(brozek~., data=fattrain, validation="CV")

  mod7ncompopt <- which.min(pls$validation$adj)
  
  pls.predict <- predict(pls, ncomp = mod7ncompopt, newdata = fattest[,2:18])
  te7 <- mean((fattest[,1] - pls.predict)^2)
  
  TEALL <- rbind(TEALL, cbind(te1, te2, te3, te4, te5, te6, te7))
  }


```

```{r cv.table}
cv_results <- data.frame("Model" = c("LRALL", "LR5", "STEP", "RIDGE", "LASSO", "PCR", "PLS"),
                         "Average Error" = 0,
                         "Average Variance" = 0)

cv_errors <- apply(TEALL, 2, mean)
cv_var <- apply(TEALL, 2, var)

for (i in 1:7){
  cv_results$Average.Error[i] <- cv_errors[i]
  cv_results$Average.Variance[i] <- cv_var[i]
}

kable(cv_results, col.names = c("Model", "Average CV Error", "Average CV Variance"), booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position"), full_width = F)
```

```{r cv.plot}
cv_results$Model <- factor(cv_results$Model, levels = c("LRALL", "LR5", "STEP", "RIDGE", "LASSO", "PCR", "PLS")) 

ggplot(cv_results, aes(x = Model, y = Average.Error)) +
  geom_point(aes(size = Average.Variance), show.legend = FALSE) +
  xlab("") +
  ylab("Average Test Error")

```

Using cross-validation, we get similar results in model performance. LASSO and LR5 had the 2 smallest average test errors, with LASSO also having a small variance in test errors (shown by the size of the dots in the above plot). STEP performed the worst and also has one of the higher variances.


## Findings

Here are some of our findings from our analysis and model building of the $fat$ dataset:

* For our response variable $brozek$ we had a couple predictors, $siri$ and $density$, that were highly correlated with it. These predictors were also consistently chosen whenever we used a variable selection method.
* The 2 models that selected the fewest predictors, LR5 and LASSO, performed the best on our intial test set and when we used Monte Carlo Cross-Validation. While using many or all of the predictors seems to do well on our training set, it did not generalize as well to our testing set as simpler models did.
* All of our testing errors were very low. Even though LASSO and LR5 did the "best", all of the methods performed very well predicting $brozek$.

On that last finding, one possible reason for this are the high correlations of $siri$ and $density$ with our response variable. This is because these predictors are extremely similar to $brozek$. In the case of $siri$, this is just another percentage of body fat equation similar to $brozek$. These equations yield similar results, and therefore the relationship between them is very strong. For $density$, this is a parameter that is used in the equations for $brozek$ and $siri$. Knowing that, it's not a surprise that $density$ would be a really good predictor for $brozek$.

Since these predictors have such a strong relationship, I wanted to see how our methods performed if we removed them from our dataset. Would the same methods that performed well before perform well now? How different will the test errors be? We once again will use Monte Carlo Cross-Validation to evaluate each method and provide the average performance below.

```{r MCCV 2}
B = 100
TEALL = NULL

set.seed(33)
for (b in 1:B){
  partition <- sort(sample(1:n, n1))
  fattrain <- fat[-partition,c(1,4:18)]
  fattest <- fat[partition,c(1,4:18)]
  
  ##LM ALL
  lm <- lm(brozek ~ ., fattrain)

  lm.predict <- predict(lm, fattest[,2:16])
  te1 <- mean((fattest[,1] - lm.predict)^2)
  
  ##LM 5
  subset <- regsubsets(brozek ~ ., fattrain, nbest= 10, really.big= TRUE)
  lm5.models <- summary(subset)$which
  lm5.models.size <- as.numeric(attr(lm5.models, "dimnames")[[1]])
  lm5.models.rss <- summary(subset)$rss
  
  op <- which(lm5.models.size == 5)
  flag2 <- op5[which.min(lm5.models.rss[op])]
  
  modselectedmodel <- lm5.models[flag2,]
  modXname <- paste(names(modselectedmodel)[modselectedmodel][-1], collapse="+")
  modform <- paste ("brozek ~", modXname)
  lm5 <- lm(as.formula(modform), data = fattrain)
  
  lm5.predict <- predict(lm5, fattest[,2:16])
  te2 <- mean((fattest[,1] - lm5.predict)^2)
  
  ##LM Stepwise
  step.model <- step(lm(brozek ~ ., fattrain), trace = FALSE)

  step.predict <- predict(step.model, fattest[,2:16])
  te3 <- mean((fattest[,1] - step.predict)^2)
  
  ##Ridge
  ridge <- lm.ridge(brozek ~ ., data = fattrain, lambda= seq(0,100,0.001))
  indexopt <-  which.min(ridge$GCV)
  
  r.coeffs = ridge$coef[,indexopt]/ ridge$scales
  r.intercept = -sum(r.coeffs  * colMeans(fattrain[,2:16] )  ) + mean(fattrain[,1])

  
  ridge.predict <- as.matrix(fattest[,2:16]) %*% as.vector(r.coeffs) + r.intercept
  te4 <- mean((fattest[,1] - ridge.predict)^2)
  
  ##LASSO
  lasso <- lars(as.matrix(fattrain[,2:16]), fattrain[,1], type= "lasso", trace= FALSE)
  Cp1  <- summary(lasso)$Cp
  index1 <- which.min(Cp1)
  
  L.coeffs <- coef(lasso)[index1,]
  L.intercept = mean(fattrain[,1]) -sum(L.coeffs  * colMeans(fattrain[,2:16]))
  
  lasso.predict <- as.matrix(fattest[,2:16]) %*% as.vector(L.coeffs) + L.intercept
  te5 <- mean((fattest[,1] - lasso.predict)^2)
  
  ##PCR
  pcr <- pcr(brozek~., data=fattrain, validation="CV")
  
  ncompopt <- which.min(pcr$validation$adj)
  
  pcr.predict <- predict(pcr, ncomp = ncompopt, newdata = fattest[,2:16])
  te6 <- mean((fattest[,1] - pcr.predict)^2)
  
  ##PLS
  pls <- plsr(brozek~., data=fattrain, validation="CV")

  mod7ncompopt <- which.min(pls$validation$adj)
  
  pls.predict <- predict(pls, ncomp = mod7ncompopt, newdata = fattest[,2:16])
  te7 <- mean((fattest[,1] - pls.predict)^2)
  
  TEALL <- rbind(TEALL, cbind(te1, te2, te3, te4, te5, te6, te7))
  }


```

```{r cv.table2}
cv_results2 <- data.frame("Model" = c("LRALL", "LR5", "STEP", "RIDGE", "LASSO", "PCR", "PLS"),
                         "Average Error" = 0,
                         "Average Variance" = 0)

cv_errors2 <- apply(TEALL, 2, mean)
cv_var2 <- apply(TEALL, 2, var)

for (i in 1:7){
  cv_results2$Average.Error[i] <- cv_errors2[i]
  cv_results2$Average.Variance[i] <- cv_var2[i]
}

kable(cv_results2, col.names = c("Model", "Average CV Error", "Average CV Variance"), booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position"), full_width = F)
```

```{r cv.plot2}
cv_results2$Model <- factor(cv_results2$Model, levels = c("LRALL", "LR5", "STEP", "RIDGE", "LASSO", "PCR", "PLS")) 

ggplot(cv_results2, aes(x = Model, y = Average.Error)) +
  geom_point(aes(size = Average.Variance), show.legend = FALSE) +
  xlab("") +
  ylab("Average Test Error")

```

The testing errors and variance increase pretty dramatically now that $siri$ and $density$ have been removed. LASSO once again is the best performing method both by average test error and variance. LR5, our other method that performed well before, is now our worst performing method by average error and variance. Makes me think that before being able to include $siri$ and $density$ was automatic when deciding on the best 5 predictors, and they carried most of the predictive power. Then with only 5 predictors any noise was removed which provided better predictions. Now with them removed LR5 might be having trouble deciding on the best 5 predictors for a training set, and therefore performs the worst and has the highest variance in testing error outcomes. Perhaps now a model using more predictors is beneficial without $siri$ and $density$ being involved.

## Appendix

Code used to perform analysis and modeling.

```{r code_appendix, eval = FALSE, echo = TRUE}
library(tidyverse)
library(corrplot)
library(leaps)
library(MASS)
library(lars)
library(pls)
library(kableExtra)

# Load data and split into train and test
fat <- read.table(file = "fat.csv", sep = ",", header = TRUE)

n = dim(fat)[1]
n1 = round(n/10)
set.seed(12)
flag = sort(sample(1:n, n1))

fat1train = fat[-flag,]
fat1test = fat[flag,]

# Exploratory Analysis
fat.mtx <- cor(fat1train)
corrplot::corrplot(fat.mtx, type = "upper", diag = FALSE)

pairs(fat1train[, c(1:3, 5, 7, 10, 11, 12)], lower.panel = NULL)

# Training and Testing of models
model1 <- lm(brozek ~ ., fat1train)

model1.predict <- predict(model1, fat1test[,2:18])
model1.te <- mean((fat1test[,1] - model1.predict)^2)

lm.subset <- regsubsets(brozek ~ ., fat1train, nbest= 10, really.big= TRUE)
model2.models <- summary(lm.subset)$which
model2.models.size <- as.numeric(attr(model2.models, "dimnames")[[1]])
model2.models.rss <- summary(lm.subset)$rss

op5 <- which(model2.models.size == 5)
flag2 <- op5[which.min(model2.models.rss[op5])]

mod2selectedmodel <- model2.models[flag2,]
mod2Xname <- paste(names(mod2selectedmodel)[mod2selectedmodel][-1], collapse="+")
mod2form <- paste ("brozek ~", mod2Xname)
model2 <- lm(as.formula(mod2form), data = fat1train)

model2.predict <- predict(model2, fat1test[,2:18])
model2.te <- mean((fat1test[,1] - model2.predict)^2)

model3  <- step(model1, trace = FALSE)

model3.predict <- predict(model3, fat1test[,2:18])
model3.te <- mean((fat1test[,1] - model3.predict)^2)

model4 <- lm.ridge(brozek ~ ., data = fat1train, lambda= seq(0,100,0.001))

indexopt <-  which.min(model4$GCV)

ridge.coeffs = model4$coef[,indexopt]/ model4$scales
intercept = -sum(ridge.coeffs  * colMeans(fat1train[,2:18] )  ) + mean(fat1train[,1])

model4.predict <- as.matrix(fat1test[,2:18]) %*% as.vector(ridge.coeffs) + intercept
model4.te <- mean((fat1test[,1] - model4.predict)^2)

model5 <- lars(as.matrix(fat1train[,2:18]), fat1train[,1], type= "lasso", trace= FALSE)

Cp1  <- summary(model5)$Cp
index1 <- which.min(Cp1)

LASSO.coeffs <- coef(model5)[index1,]

LASSOintercept = mean(fat1train[,1]) -sum(LASSO.coeffs  * colMeans(fat1train[,2:18]))

model5.predict <- as.matrix(fat1test[,2:18]) %*% as.vector(LASSO.coeffs) + LASSOintercept
model5.te <- mean((fat1test[,1] - model5.predict)^2)

set.seed(33)
model6 <- pcr(brozek~., data=fat1train, validation="CV")

ncompopt <- which.min(model6$validation$adj)

model6.predict <- predict(model6, ncomp = ncompopt, newdata = fat1test[,2:18])
model6.te <- mean((fat1test[,1] - model6.predict)^2)

set.seed(33)
model7 <- plsr(brozek~., data=fat1train, validation="CV")

mod7ncompopt <- which.min(model7$validation$adj)

model7.predict <- predict(model7, ncomp = mod7ncompopt, newdata = fat1test[,2:18])
model7.te <- mean((fat1test[,1] - model7.predict)^2)

# Results and Cross Validation
methods <- c("LRALL", "LR5", "STEP", "RIDGE", "LASSO", "PCR", "PLS")
errors <- c(model1.te, model2.te, model3.te, model4.te, model5.te, model6.te, model7.te)

table <- data.frame(methods, errors)

kable(table, col.names = c("Methods", "Mean Squared Error")) %>%
  kable_styling(position = "center", full_width = F)

B = 100
TEALL = NULL

set.seed(33)
for (b in 1:B){
  partition <- sort(sample(1:n, n1))
  fattrain <- fat[-partition,]
  fattest <- fat[partition,]
  
  ##LM ALL
  lm <- lm(brozek ~ ., fattrain)

  lm.predict <- predict(lm, fattest[,2:18])
  te1 <- mean((fattest[,1] - lm.predict)^2)
  
  ##LM 5
  subset <- regsubsets(brozek ~ ., fattrain, nbest= 10, really.big= TRUE)
  lm5.models <- summary(subset)$which
  lm5.models.size <- as.numeric(attr(lm5.models, "dimnames")[[1]])
  lm5.models.rss <- summary(subset)$rss
  
  op <- which(lm5.models.size == 5)
  flag2 <- op5[which.min(lm5.models.rss[op])]
  
  modselectedmodel <- lm5.models[flag2,]
  modXname <- paste(names(modselectedmodel)[modselectedmodel][-1], collapse="+")
  modform <- paste ("brozek ~", modXname)
  lm5 <- lm(as.formula(modform), data = fattrain)
  
  lm5.predict <- predict(lm5, fattest[,2:18])
  te2 <- mean((fattest[,1] - lm5.predict)^2)
  
  ##LM Stepwise
  step.model <- step(lm(brozek ~ ., fattrain), trace = FALSE)

  step.predict <- predict(step.model, fattest[,2:18])
  te3 <- mean((fattest[,1] - step.predict)^2)
  
  ##Ridge
  ridge <- lm.ridge(brozek ~ ., data = fattrain, lambda= seq(0,100,0.001))
  indexopt <-  which.min(ridge$GCV)
  
  r.coeffs = ridge$coef[,indexopt]/ ridge$scales
  r.intercept = -sum(r.coeffs  * colMeans(fattrain[,2:18] )  ) + mean(fattrain[,1])

  
  ridge.predict <- as.matrix(fattest[,2:18]) %*% as.vector(r.coeffs) + r.intercept
  te4 <- mean((fattest[,1] - ridge.predict)^2)
  
  ##LASSO
  lasso <- lars(as.matrix(fattrain[,2:18]), fattrain[,1], type= "lasso", trace= FALSE)
  Cp1  <- summary(lasso)$Cp
  index1 <- which.min(Cp1)
  
  L.coeffs <- coef(lasso)[index1,]
  L.intercept = mean(fattrain[,1]) -sum(L.coeffs  * colMeans(fattrain[,2:18]))
  
  lasso.predict <- as.matrix(fattest[,2:18]) %*% as.vector(L.coeffs) + L.intercept
  te5 <- mean((fattest[,1] - lasso.predict)^2)
  
  ##PCR
  pcr <- pcr(brozek~., data=fattrain, validation="CV")
  
  ncompopt <- which.min(pcr$validation$adj)
  
  pcr.predict <- predict(pcr, ncomp = ncompopt, newdata = fattest[,2:18])
  te6 <- mean((fattest[,1] - pcr.predict)^2)
  
  ##PLS
  pls <- plsr(brozek~., data=fattrain, validation="CV")

  mod7ncompopt <- which.min(pls$validation$adj)
  
  pls.predict <- predict(pls, ncomp = mod7ncompopt, newdata = fattest[,2:18])
  te7 <- mean((fattest[,1] - pls.predict)^2)
  
  TEALL <- rbind(TEALL, cbind(te1, te2, te3, te4, te5, te6, te7))
  }


cv_results <- data.frame("Model" = c("LRALL", "LR5", "STEP", "RIDGE", "LASSO", "PCR", "PLS"),
                         "Average Error" = 0,
                         "Average Variance" = 0)

cv_errors <- apply(TEALL, 2, mean)
cv_var <- apply(TEALL, 2, var)

for (i in 1:7){
  cv_results$Average.Error[i] <- cv_errors[i]
  cv_results$Average.Variance[i] <- cv_var[i]
}

kable(cv_results, col.names = c("Model", "Average CV Error", "Average CV Variance")) %>%
  kable_styling(position = "center", full_width = F)

cv_results$Model <- factor(cv_results$Model, levels = c("LRALL", "LR5", "STEP", "RIDGE", "LASSO", "PCR", "PLS")) 

ggplot(cv_results, aes(x = Model, y = Average.Error)) +
  geom_point(aes(size = Average.Variance), show.legend = FALSE) +
  xlab("") +
  ylab("Average Test Error")

B = 100
TEALL = NULL

set.seed(33)
for (b in 1:B){
  partition <- sort(sample(1:n, n1))
  fattrain <- fat[-partition,c(1,4:18)]
  fattest <- fat[partition,c(1,4:18)]
  
  ##LM ALL
  lm <- lm(brozek ~ ., fattrain)

  lm.predict <- predict(lm, fattest[,2:16])
  te1 <- mean((fattest[,1] - lm.predict)^2)
  
  ##LM 5
  subset <- regsubsets(brozek ~ ., fattrain, nbest= 10, really.big= TRUE)
  lm5.models <- summary(subset)$which
  lm5.models.size <- as.numeric(attr(lm5.models, "dimnames")[[1]])
  lm5.models.rss <- summary(subset)$rss
  
  op <- which(lm5.models.size == 5)
  flag2 <- op5[which.min(lm5.models.rss[op])]
  
  modselectedmodel <- lm5.models[flag2,]
  modXname <- paste(names(modselectedmodel)[modselectedmodel][-1], collapse="+")
  modform <- paste ("brozek ~", modXname)
  lm5 <- lm(as.formula(modform), data = fattrain)
  
  lm5.predict <- predict(lm5, fattest[,2:16])
  te2 <- mean((fattest[,1] - lm5.predict)^2)
  
  ##LM Stepwise
  step.model <- step(lm(brozek ~ ., fattrain), trace = FALSE)

  step.predict <- predict(step.model, fattest[,2:16])
  te3 <- mean((fattest[,1] - step.predict)^2)
  
  ##Ridge
  ridge <- lm.ridge(brozek ~ ., data = fattrain, lambda= seq(0,100,0.001))
  indexopt <-  which.min(ridge$GCV)
  
  r.coeffs = ridge$coef[,indexopt]/ ridge$scales
  r.intercept = -sum(r.coeffs  * colMeans(fattrain[,2:16] )  ) + mean(fattrain[,1])

  
  ridge.predict <- as.matrix(fattest[,2:16]) %*% as.vector(r.coeffs) + r.intercept
  te4 <- mean((fattest[,1] - ridge.predict)^2)
  
  ##LASSO
  lasso <- lars(as.matrix(fattrain[,2:16]), fattrain[,1], type= "lasso", trace= FALSE)
  Cp1  <- summary(lasso)$Cp
  index1 <- which.min(Cp1)
  
  L.coeffs <- coef(lasso)[index1,]
  L.intercept = mean(fattrain[,1]) -sum(L.coeffs  * colMeans(fattrain[,2:16]))
  
  lasso.predict <- as.matrix(fattest[,2:16]) %*% as.vector(L.coeffs) + L.intercept
  te5 <- mean((fattest[,1] - lasso.predict)^2)
  
  ##PCR
  pcr <- pcr(brozek~., data=fattrain, validation="CV")
  
  ncompopt <- which.min(pcr$validation$adj)
  
  pcr.predict <- predict(pcr, ncomp = ncompopt, newdata = fattest[,2:16])
  te6 <- mean((fattest[,1] - pcr.predict)^2)
  
  ##PLS
  pls <- plsr(brozek~., data=fattrain, validation="CV")

  mod7ncompopt <- which.min(pls$validation$adj)
  
  pls.predict <- predict(pls, ncomp = mod7ncompopt, newdata = fattest[,2:16])
  te7 <- mean((fattest[,1] - pls.predict)^2)
  
  TEALL <- rbind(TEALL, cbind(te1, te2, te3, te4, te5, te6, te7))
  }

cv_results2 <- data.frame("Model" = c("LRALL", "LR5", "STEP", "RIDGE", "LASSO", "PCR", "PLS"),
                         "Average Error" = 0,
                         "Average Variance" = 0)

cv_errors2 <- apply(TEALL, 2, mean)
cv_var2 <- apply(TEALL, 2, var)

for (i in 1:7){
  cv_results2$Average.Error[i] <- cv_errors2[i]
  cv_results2$Average.Variance[i] <- cv_var2[i]
}

kable(cv_results2, col.names = c("Model", "Average CV Error", "Average CV Variance")) %>%
  kable_styling(position = "center", full_width = F)

cv_results2$Model <- factor(cv_results2$Model, levels = c("LRALL", "LR5", "STEP", "RIDGE", "LASSO", "PCR", "PLS")) 

ggplot(cv_results2, aes(x = Model, y = Average.Error)) +
  geom_point(aes(size = Average.Variance), show.legend = FALSE) +
  xlab("") +
  ylab("Average Test Error")
```


